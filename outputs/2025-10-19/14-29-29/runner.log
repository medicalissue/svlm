[2025-10-19 14:29:29,095][__main__][INFO] - Loaded configuration for run 'baseline'
[2025-10-19 14:29:29,095][__main__][INFO] - Overriding model device to cuda:0
[2025-10-19 14:29:29,353][__main__][INFO] - Set torch CUDA device to 0
[2025-10-19 14:29:29,354][__main__][INFO] - Loaded adapter via fallback path 'src.svlm.model_adapter'
[2025-10-19 14:29:37,921][__main__][INFO] - Cross-attention blocks (30 total):
[2025-10-19 14:29:37,921][__main__][INFO] -   - layer=0, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=1, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=2, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=3, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=4, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=5, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=6, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=7, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=8, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=9, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=10, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=11, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=12, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=13, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=14, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=15, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=16, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=17, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=18, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=19, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=20, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=21, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=22, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=23, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=24, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=25, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=26, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=27, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=vision, type=vision_encoder, heads=None, hidden=1536, extra={'image_size': None, 'note': 'Vision encoder configuration'}
[2025-10-19 14:29:37,922][__main__][INFO] -   - layer=multimodal, type=cross_attention, heads=None, extra={'note': 'Qwen2-VL multimodal fusion layers (approximate)'}
[2025-10-19 14:29:38,058][src.svlm.pipeline][WARNING] - Cache directory '/data/coco' is not writable; falling back to default HuggingFace cache.
[2025-10-19 14:29:43,646][__main__][INFO] - Prompt: Is there a snowboard in the image?
[2025-10-19 14:29:43,646][__main__][INFO] - Output: There is no existence of a snowboard in the image description.
[2025-10-19 14:29:44,469][__main__][INFO] - Prompt: Is there a backpack in the image?
[2025-10-19 14:29:44,469][__main__][INFO] - Output: There is no existence of a backpack in the image.
[2025-10-19 14:29:45,216][__main__][INFO] - Prompt: Is there a person in the image?
[2025-10-19 14:29:45,216][__main__][INFO] - Output: Yes, there is a person in the image.
[2025-10-19 14:29:46,101][__main__][INFO] - Prompt: Is there a car in the image?
[2025-10-19 14:29:46,101][__main__][INFO] - Output: There is no existence of a car in the image description.
[2025-10-19 14:29:46,982][__main__][INFO] - Prompt: Is there a skis in the image?
[2025-10-19 14:29:46,982][__main__][INFO] - Output: There is no existence of skis in the image description.
[2025-10-19 14:29:47,864][__main__][INFO] - Prompt: Is there a dog in the image?
[2025-10-19 14:29:47,865][__main__][INFO] - Output: There is no existence of a dog in the image description.
[2025-10-19 14:29:48,407][__main__][INFO] - Prompt: Is there a truck in the image?
[2025-10-19 14:29:48,408][__main__][INFO] - Output: There is no existence of a truck in the image.
[2025-10-19 14:29:48,939][__main__][INFO] - Prompt: Is there a car in the image?
[2025-10-19 14:29:48,939][__main__][INFO] - Output: There is no existence of a car in the image.
[2025-10-19 14:29:49,434][__main__][INFO] - Prompt: Is there a person in the image?
[2025-10-19 14:29:49,434][__main__][INFO] - Output: Yes, there is a person in the image.
[2025-10-19 14:29:50,061][__main__][INFO] - Prompt: Is there a dining table in the image?
[2025-10-19 14:29:50,061][__main__][INFO] - Output: There is no existence of a dining table in the image description.
[2025-10-19 14:29:50,551][__main__][INFO] - Prompt: Is there an umbrella in the imange?
[2025-10-19 14:29:50,552][__main__][INFO] - Output: Yes, there is an umbrella in the image.
[2025-10-19 14:29:51,174][__main__][INFO] - Prompt: Is there a handbag in the image?
[2025-10-19 14:29:51,174][__main__][INFO] - Output: There is no existence of a handbag in the image description.
[2025-10-19 14:29:51,995][__main__][INFO] - Prompt: Is there a person in the image?
[2025-10-19 14:29:51,995][__main__][INFO] - Output: There is no existence of any person in the image.
[2025-10-19 14:29:52,969][__main__][INFO] - Prompt: Is there a dining table in the image?
[2025-10-19 14:29:52,969][__main__][INFO] - Output: There is no existence of a dining table in the image descriptions.
[2025-10-19 14:29:53,733][__main__][INFO] - Prompt: Is there a bicycle in the image?
[2025-10-19 14:29:53,733][__main__][INFO] - Output: Yes, there is a bicycle in the image.
[2025-10-19 14:29:54,622][__main__][INFO] - Prompt: Is there a motorcycle in the image?
[2025-10-19 14:29:54,622][__main__][INFO] - Output: There is no existence of a motorcycle in the image description.
[2025-10-19 14:29:54,623][__main__][INFO] - Evaluation metrics: {'pope_accuracy': 0.25, 'pope_yes_rate': 0.25, 'pope_f1': 0.6666666666666666, 'chair_i': 0.0, 'chair_s': 0.0}
