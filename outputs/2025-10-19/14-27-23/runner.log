[2025-10-19 14:27:23,596][__main__][INFO] - Loaded configuration for run 'baseline'
[2025-10-19 14:27:23,596][__main__][INFO] - Overriding model device to cuda:0
[2025-10-19 14:27:23,851][__main__][INFO] - Set torch CUDA device to 0
[2025-10-19 14:27:23,851][__main__][INFO] - Loaded adapter via fallback path 'src.svlm.model_adapter'
[2025-10-19 14:27:32,391][__main__][INFO] - Cross-attention blocks (30 total):
[2025-10-19 14:27:32,391][__main__][INFO] -   - layer=0, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=1, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=2, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=3, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=4, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=5, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=6, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=7, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=8, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=9, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=10, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=11, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=12, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=13, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=14, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=15, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=16, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=17, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=18, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=19, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=20, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=21, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=22, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=23, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=24, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=25, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=26, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=27, type=self_attention, heads=12, hidden=1536, extra={'note': 'Transformer text block'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=vision, type=vision_encoder, heads=None, hidden=1536, extra={'image_size': None, 'note': 'Vision encoder configuration'}
[2025-10-19 14:27:32,392][__main__][INFO] -   - layer=multimodal, type=cross_attention, heads=None, extra={'note': 'Qwen2-VL multimodal fusion layers (approximate)'}
[2025-10-19 14:27:32,523][src.svlm.pipeline][WARNING] - Cache directory '/data/coco' is not writable; falling back to default HuggingFace cache.
[2025-10-19 14:27:38,219][__main__][INFO] - Prompt: Is there a snowboard in the image?
[2025-10-19 14:27:38,219][__main__][INFO] - Output: There is no existence of a snowboard in the image description.
[2025-10-19 14:27:39,063][__main__][INFO] - Prompt: Is there a backpack in the image?
[2025-10-19 14:27:39,063][__main__][INFO] - Output: There is no existence of a backpack in the image.
[2025-10-19 14:27:39,827][__main__][INFO] - Prompt: Is there a person in the image?
[2025-10-19 14:27:39,827][__main__][INFO] - Output: Yes, there is a person in the image.
[2025-10-19 14:27:40,664][__main__][INFO] - Prompt: Is there a car in the image?
[2025-10-19 14:27:40,664][__main__][INFO] - Output: There is no existence of a car in the image.
[2025-10-19 14:27:41,571][__main__][INFO] - Prompt: Is there a skis in the image?
[2025-10-19 14:27:41,571][__main__][INFO] - Output: There is no existence of any skis in the image.
[2025-10-19 14:27:42,469][__main__][INFO] - Prompt: Is there a dog in the image?
[2025-10-19 14:27:42,469][__main__][INFO] - Output: There is no existence of a dog in the image information.
[2025-10-19 14:27:43,015][__main__][INFO] - Prompt: Is there a truck in the image?
[2025-10-19 14:27:43,015][__main__][INFO] - Output: There is no existence of a truck in the image.
[2025-10-19 14:27:43,424][__main__][INFO] - Prompt: Is there a car in the image?
[2025-10-19 14:27:43,425][__main__][INFO] - Output: There is a car in the image.
[2025-10-19 14:27:43,880][__main__][INFO] - Prompt: Is there a person in the image?
[2025-10-19 14:27:43,880][__main__][INFO] - Output: There is a person standing in the image.
[2025-10-19 14:27:44,469][__main__][INFO] - Prompt: Is there a dining table in the image?
[2025-10-19 14:27:44,469][__main__][INFO] - Output: There is no existence of a dining table in the image.
[2025-10-19 14:27:45,990][__main__][INFO] - Prompt: Is there an umbrella in the imange?
[2025-10-19 14:27:45,991][__main__][INFO] - Output: Yes, there is an umbrella in the image. It is being held by an adult and is open, providing protection from the rain.
[2025-10-19 14:27:46,592][__main__][INFO] - Prompt: Is there a handbag in the image?
[2025-10-19 14:27:46,592][__main__][INFO] - Output: There is no existence of a handbag in the image.
[2025-10-19 14:27:47,235][__main__][INFO] - Prompt: Is there a person in the image?
[2025-10-19 14:27:47,235][__main__][INFO] - Output: There is a person in the image.
[2025-10-19 14:27:48,419][__main__][INFO] - Prompt: Is there a dining table in the image?
[2025-10-19 14:27:48,419][__main__][INFO] - Output: There is no existence of a dining table in the given information about the image.
[2025-10-19 14:27:49,187][__main__][INFO] - Prompt: Is there a bicycle in the image?
[2025-10-19 14:27:49,187][__main__][INFO] - Output: Yes, there is a bicycle in the image.
[2025-10-19 14:27:50,102][__main__][INFO] - Prompt: Is there a motorcycle in the image?
[2025-10-19 14:27:50,103][__main__][INFO] - Output: There is no existence of a motorcycle in the image description.
[2025-10-19 14:27:50,103][__main__][INFO] - Evaluation metrics: {'pope_accuracy': 0.0, 'pope_yes_rate': 0.1875, 'pope_f1': 0.0, 'chair_i': 0.0, 'chair_s': 0.0}
